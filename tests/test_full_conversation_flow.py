"""
å®Œæ•´çš„ç«¯åˆ°ç«¯å¯¹è¯æµç¨‹æµ‹è¯•
è®©çœŸå®çš„LLMå¤„ç†Classifier->Filter->Summarizerçš„å®Œæ•´æµç¨‹
"""

import asyncio
import json
import sys
import os
import aiohttp
from typing import Dict, Any, Optional, List

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# æ¨¡æ‹Ÿ astrbot æ¨¡å—
sys.modules['astrbot'] = type(sys)('astrbot')
sys.modules['astrbot.api'] = type(sys)('astrbot.api')
sys.modules['astrbot.api.provider'] = type(sys)('astrbot.api.provider')


class LocalLLMProvider:
    """
    æœ¬åœ°LLM Providerï¼Œè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„Geminiæ¨¡å‹
    """
    def __init__(self, base_url: str = "http://127.0.0.1:7861", api_key: str = "123qwe", model: str = "gemini-2.5-flash-lite"):
        """
        åˆå§‹åŒ–æœ¬åœ°LLM Provider

        Args:
            base_url: æœ¬åœ°LLMæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    async def text_chat(self, prompt: str, **kwargs) -> Any:
        """
        è°ƒç”¨æœ¬åœ°LLMè¿›è¡Œæ–‡æœ¬å¯¹è¯

        Args:
            prompt: è¾“å…¥æç¤ºè¯

        Returns:
            åŒ…å«completion_textçš„å“åº”å¯¹è±¡
        """
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 2000)
        }

        # å‘é€è¯·æ±‚åˆ°æœ¬åœ°LLM
        async with aiohttp.ClientSession() as session:
            try:
                url = f"{self.base_url}/v1/chat/completions"
                print(f"æ­£åœ¨è°ƒç”¨æœ¬åœ°LLM: {url}")

                async with session.post(url, json=data, headers=self.headers) as response:
                    if response.status == 200:
                        result = await response.json()
                        # æå–å“åº”æ–‡æœ¬
                        completion_text = result.get("choices", [{}])[0].get("message", {}).get("content", "")

                        # åˆ›å»ºå“åº”å¯¹è±¡
                        response_obj = type('Response', (), {})()
                        response_obj.completion_text = completion_text

                        print(f"LLMå“åº”é•¿åº¦: {len(completion_text)} å­—ç¬¦")
                        return response_obj
                    else:
                        error_text = await response.text()
                        print(f"LLMè°ƒç”¨å¤±è´¥ ({response.status}): {error_text}")
                        # è¿”å›ä¸€ä¸ªç©ºå“åº”
                        response_obj = type('Response', (), {})()
                        response_obj.completion_text = ""
                        return response_obj

            except Exception as e:
                print(f"è°ƒç”¨LLMæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # è¿”å›ä¸€ä¸ªç©ºå“åº”
                response_obj = type('Response', (), {})()
                response_obj.completion_text = ""
                return response_obj


# æ³¨å…¥ProvideråŸºç±»
sys.modules['astrbot.api.provider'].Provider = LocalLLMProvider

from ..models.request import KnowledgeRequest
from ..roles.classifier import Classifier
from ..roles.smart_retriever import SmartRetriever


async def test_conversation_scenario(conversation_history: List[Dict], current_prompt: str, scenario_name: str):
    """
    æµ‹è¯•ä¸€ä¸ªå¯¹è¯åœºæ™¯

    Args:
        conversation_history: å¯¹è¯å†å²
        current_prompt: å½“å‰ç”¨æˆ·è¾“å…¥
        scenario_name: åœºæ™¯åç§°

    Returns:
        bool: æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•åœºæ™¯: {scenario_name}")
    print(f"{'='*60}")

    # 1. åˆ›å»ºæœ¬åœ°LLM Provider
    print("\n1. åˆå§‹åŒ–æœ¬åœ°LLM Provider")
    local_llm = LocalLLMProvider(
        base_url="http://127.0.0.1:7861",
        api_key="",
        model="gemini-2.5-flash-lite"
    )
    print("   âœ“ Provideråˆå§‹åŒ–å®Œæˆ")

    # 2. åˆå§‹åŒ–Classifier
    print("\n2. åˆå§‹åŒ–Classifier")
    try:
        classifier = Classifier(provider=local_llm)
        print("   âœ“ Classifieråˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"   âœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

    # 3. åˆå§‹åŒ–SmartRetriever
    print("\n3. åˆå§‹åŒ–SmartRetriever")
    config = {
        "moegirl_enabled": True,
        "wikipedia_enabled": True,
        "wikidata_enabled": True,
        "retrieval": {
            "text_length_threshold": 500,
            "max_search_results": 5
        }
    }

    try:
        retriever = SmartRetriever(analyzer_provider=local_llm, config=config)
        print("   âœ“ SmartRetrieveråˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"   âœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

    # 4. æ‰§è¡ŒClassifieråˆ†æ
    print(f"\n4. æ‰§è¡ŒClassifieråˆ†æ")
    print(f"   ç”¨æˆ·è¾“å…¥: {current_prompt}")

    try:
        knowledge_request = await classifier.classify(conversation_history, current_prompt)
        print("   âœ“ Classifieråˆ†æå®Œæˆ")

        if knowledge_request:
            print(f"   ç”Ÿæˆçš„çŸ¥è¯†è¯·æ±‚:")
            print(f"     æ–‡æ¡£æŸ¥è¯¢: {knowledge_request.required_docs}")
            print(f"     äº‹å®æŸ¥è¯¢: {knowledge_request.required_facts}")
        else:
            print("   æœªç”ŸæˆçŸ¥è¯†è¯·æ±‚ï¼ˆæ— éœ€æŸ¥è¯¢ï¼‰")
            return True

    except Exception as e:
        print(f"   âœ— Classifieråˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 5. æ‰§è¡Œæ£€ç´¢ï¼ˆä½¿ç”¨çœŸå®LLMï¼‰
    print("\n5. æ‰§è¡Œæ£€ç´¢æµç¨‹")
    print("   æ³¨æ„ï¼šè¿™å°†ä½¿ç”¨çœŸå®çš„LLMè¿›è¡ŒFilterå’ŒSummarizeræ“ä½œ")
    print("-" * 50)

    try:
        knowledge_result = await retriever.retrieve(knowledge_request)
        print("\n   âœ“ æ£€ç´¢å®Œæˆ")
    except Exception as e:
        print(f"\n   âœ— æ£€ç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 6. å±•ç¤ºç»“æœ
    print(f"\n{'='*60}")
    print("æ£€ç´¢ç»“æœ")
    print(f"{'='*60}")

    if not knowledge_result or not knowledge_result.chunks:
        print("   âœ— æœªè¿”å›ä»»ä½•ç»“æœ")
        return False

    print(f"\nå…±è·å–åˆ° {len(knowledge_result.chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ:\n")

    for i, chunk in enumerate(knowledge_result.chunks, 1):
        print(f"\nã€ç‰‡æ®µ {i}ã€‘")
        print(f"æ¥æº: {chunk.source}")
        print(f"å®ä½“: {chunk.entity}")
        if chunk.source_url:
            print(f"URL: {chunk.source_url}")

        # æ‰“å°å†…å®¹é¢„è§ˆ
        content_preview = chunk.content[:200] if len(chunk.content) > 200 else chunk.content
        print(f"å†…å®¹é¢„è§ˆ:\n{content_preview}")
        if len(chunk.content) > 200:
            print(f"... (å…± {len(chunk.content)} å­—ç¬¦)")

        print("-" * 50)

    print(f"\nâœ… åœºæ™¯ '{scenario_name}' æµ‹è¯•æˆåŠŸï¼")
    return True


async def run_all_conversation_tests():
    """
    è¿è¡Œæ‰€æœ‰å¯¹è¯åœºæ™¯æµ‹è¯•
    """
    print("\n" + "="*80)
    print("å¼€å§‹å®Œæ•´çš„ç«¯åˆ°ç«¯å¯¹è¯æµç¨‹æµ‹è¯•")
    print("ä½¿ç”¨çœŸå®çš„æœ¬åœ°LLMå¤„ç†Classifier->Filter->Summarizerå®Œæ•´æµç¨‹")
    print("="*80)

    # å®šä¹‰å¤šä¸ªæµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            "name": "æ¸¸æˆæŸ¥è¯¢åœºæ™¯",
            "history": [
                {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹åŸç¥è¿™ä¸ªæ¸¸æˆ"},
                {"role": "assistant", "content": "åŸç¥æ˜¯ç”±ç±³å“ˆæ¸¸å¼€å‘çš„ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œå†’é™©æ¸¸æˆ"}
            ],
            "prompt": "èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹åŸç¥çš„èƒŒæ™¯æ•…äº‹å’Œä¸»è¦è§’è‰²å—ï¼Ÿ"
        },
        {
            "name": "å†å²æŸ¥è¯¢åœºæ™¯",
            "history": [
                {"role": "user", "content": "æˆ‘åœ¨çœ‹ä¸€äº›å†å²çºªå½•ç‰‡"},
                {"role": "assistant", "content": "å†å²æ˜¯å¾ˆæœ‰è¶£çš„ä¸»é¢˜"}
            ],
            "prompt": "å¸®æˆ‘æŸ¥ä¸€ä¸‹é•¿åŸçš„å†å²å’Œå»ºé€ èƒŒæ™¯"
        },
        {
            "name": "åœ°ç†äº‹å®æŸ¥è¯¢åœºæ™¯",
            "history": [
                {"role": "user", "content": "æˆ‘åœ¨å­¦ä¹ åœ°ç†çŸ¥è¯†"},
                {"role": "assistant", "content": "åœ°ç†å­¦æ¶‰åŠåœ°çƒè¡¨é¢çš„è‡ªç„¶å’Œäººæ–‡ç°è±¡"}
            ],
            "prompt": "çº½çº¦çš„åœ°ç†åæ ‡æ˜¯å¤šå°‘ï¼Ÿ"
        },
        {
            "name": "æ··åˆæŸ¥è¯¢åœºæ™¯",
            "history": [
                {"role": "user", "content": "æˆ‘æœ€è¿‘åœ¨ç ”ç©¶ä¸€äº›æ–‡åŒ–å†…å®¹"},
                {"role": "assistant", "content": "æ–‡åŒ–ç ”ç©¶æ¶‰åŠå¾ˆå¤šæœ‰è¶£çš„é¢†åŸŸ"}
            ],
            "prompt": "æˆ‘æƒ³äº†è§£æ¥è‡ªæ·±æ¸Šè¿™éƒ¨åŠ¨æ¼«ï¼Œè¿˜æœ‰æœ±ç¥é•‡è¿™ä¸ªå†å²äººç‰©"
        },
        {
            "name": "æ—¥å¸¸å¯¹è¯åœºæ™¯",
            "history": [
                {"role": "user", "content": "ä»Šå¤©çš„å¤©æ°”çœŸå¥½"},
                {"role": "assistant", "content": "æ˜¯çš„ï¼Œé€‚åˆå‡ºå»èµ°èµ°"}
            ],
            "prompt": "ä½ è§‰å¾—å»å“ªé‡Œæ•£æ­¥æ¯”è¾ƒå¥½ï¼Ÿ"
        }
    ]

    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    passed = 0
    failed = 0

    for scenario in test_scenarios:
        try:
            success = await test_conversation_scenario(
                scenario["history"],
                scenario["prompt"],
                scenario["name"]
            )
            if success:
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"\nâŒ åœºæ™¯ '{scenario['name']}' æ‰§è¡Œå‡ºé”™: {e}")
            failed += 1
            import traceback
            traceback.print_exc()

    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*80}")
    print("å®Œæ•´å¯¹è¯æµç¨‹æµ‹è¯•æ€»ç»“")
    print(f"{'='*80}")
    print(f"æ€»åœºæ™¯æ•°: {len(test_scenarios)}")
    print(f"âœ… æˆåŠŸ: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")

    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰å¯¹è¯åœºæ™¯æµ‹è¯•é€šè¿‡ï¼")
        print("è¯´æ˜ï¼šç³»ç»ŸæˆåŠŸå®Œæˆäº†ä»å¯¹è¯ç†è§£åˆ°çŸ¥è¯†æ£€ç´¢çš„å®Œæ•´æµç¨‹")
        print("åŒ…æ‹¬ï¼šClassifierç”ŸæˆJSONã€Filterç­›é€‰ã€Summarizeræ‘˜è¦")
    else:
        print(f"\nâš ï¸ æœ‰ {failed} ä¸ªåœºæ™¯æµ‹è¯•å¤±è´¥")

    print(f"{'='*80}")

    return failed == 0


async def test_single_classifier_only():
    """
    ä»…æµ‹è¯•Classifierçš„JSONç”Ÿæˆèƒ½åŠ›
    """
    print("\n" + "="*60)
    print("Classifier JSONç”Ÿæˆèƒ½åŠ›æµ‹è¯•")
    print("="*60)

    # åˆ›å»ºæœ¬åœ°LLM Provider
    local_llm = LocalLLMProvider()
    classifier = Classifier(provider=local_llm)

    # æµ‹è¯•ä¸åŒçš„è¾“å…¥
    test_inputs = [
        {
            "history": [],
            "prompt": "å¸®æˆ‘æŸ¥ä¸€ä¸‹åŸç¥æ˜¯ä»€ä¹ˆæ¸¸æˆï¼Œè¿˜æœ‰çº½çº¦çš„åæ ‡",
            "description": "æ··åˆæŸ¥è¯¢ï¼ˆæ¸¸æˆ+åœ°ç†äº‹å®ï¼‰"
        },
        {
            "history": [{"role": "user", "content": "æˆ‘åœ¨å­¦ä¹ å†å²"}, {"role": "assistant", "content": "å†å²å¾ˆæœ‰è¶£"}],
            "prompt": "æœ±ç¥é•‡çš„çˆ¶äº²æ˜¯è°ï¼Ÿ",
            "description": "å†å²äººç‰©äº‹å®æŸ¥è¯¢"
        },
        {
            "history": [],
            "prompt": "ä»Šå¤©å¤©æ°”ä¸é”™",
            "description": "æ—¥å¸¸å¯¹è¯ï¼ˆæ— éœ€æŸ¥è¯¢ï¼‰"
        }
    ]

    for i, test_input in enumerate(test_inputs, 1):
        print(f"\næµ‹è¯• {i}: {test_input['description']}")
        print(f"è¾“å…¥: {test_input['prompt']}")

        try:
            result = await classifier.classify(test_input["history"], test_input["prompt"])
            if result:
                print(f"ç”Ÿæˆçš„JSON:")
                print(f"  required_docs: {result.required_docs}")
                print(f"  required_facts: {result.required_facts}")
            else:
                print("  ç»“æœ: æ— éœ€æŸ¥è¯¢")
        except Exception as e:
            print(f"  é”™è¯¯: {e}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å®Œæ•´å¯¹è¯æµç¨‹æµ‹è¯•")
    parser.add_argument("--classifier-only", action="store_true", help="ä»…æµ‹è¯•Classifier")
    parser.add_argument("--scenario", type=str, help="è¿è¡Œç‰¹å®šåœºæ™¯")
    args = parser.parse_args()

    if args.classifier_only:
        # ä»…æµ‹è¯•Classifier
        asyncio.run(test_single_classifier_only())
    elif args.scenario:
        # è¿è¡Œç‰¹å®šåœºæ™¯
        print("æš‚ä¸æ”¯æŒç‰¹å®šåœºæ™¯è¿è¡Œ")
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        result = asyncio.run(run_all_conversation_tests())
        exit(0 if result else 1)